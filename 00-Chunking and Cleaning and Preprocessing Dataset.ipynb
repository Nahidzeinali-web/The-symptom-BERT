{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e455bfc-d89a-433b-92ee-484cc673cc3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chunking Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58cb835-ade5-4d67-ab5a-4f5957bd73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import re  # Import the re module for regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13804236-adb4-4ab4-a9d2-e541e644ccf2",
   "metadata": {},
   "source": [
    "### GPU Configuration in PyTorch\n",
    "\n",
    "This section outlines how to check for GPU availability and configure PyTorch to use the GPU if available, otherwise fall back to using the CPU. This is crucial for optimizing computational efficiency, especially when working with large neural network models that benefit significantly from GPU acceleration.\n",
    "\n",
    "#### Code Explanation:\n",
    "\n",
    "1. **Check GPU Availability**:\n",
    "   - The code begins by checking if a CUDA-compatible GPU is available using `torch.cuda.is_available()`. CUDA is NVIDIA's parallel computing architecture, which allows PyTorch to accelerate operations using the GPU.\n",
    "\n",
    "2. **Configure Device**:\n",
    "   - If a GPU is available:\n",
    "     - The code sets the `device` to use CUDA by calling `torch.device(\"cuda\")`.\n",
    "     - It prints out the number of GPUs available using `torch.cuda.device_count()` and displays the name of the GPU that will be used, which is particularly useful for ensuring that the expected hardware is being utilized.\n",
    "   - If no GPU is available:\n",
    "     - The code outputs a message indicating that no GPU is available and sets the `device` to use the CPU instead. This ensures that the code remains portable and can run on systems without a GPU, albeit at potentially lower speeds.\n",
    "\n",
    "#### Code Usage:\n",
    "This setup is typically employed at the beginning of a script to configure the hardware settings appropriately before proceeding with data loading, model creation, and training processes. It helps in leveraging available hardware to its fullest potential, or provides a fallback to ensure compatibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd43dcf0-f743-46a6-895c-46b3d27b6b57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee5390-7875-4ca0-96a4-77f3871c65e0",
   "metadata": {},
   "source": [
    "### Processing Large CSV Files in Chunks Using Pandas\n",
    "\n",
    "This guide details how to efficiently process a large CSV file in manageable chunks using Python's pandas library. This approach is particularly useful when dealing with large datasets that do not fit into memory.\n",
    "\n",
    "#### Code Overview:\n",
    "\n",
    "1. **Parameters Setup**:\n",
    "   - **input_file**: Specifies the path to the CSV file to be processed.\n",
    "   - **chunk_size**: Determines the number of rows each chunk should contain. It is dynamically calculated as the total number of rows in the CSV file, allowing for evenly sized chunks except for the last one.\n",
    "   - **number_of_chunks**: Sets the total number of chunks to be processed and saved.\n",
    "\n",
    "2. **Chunk Processing**:\n",
    "   - The code iterates through the CSV file in segments, each limited to `chunk_size` rows.\n",
    "   - Each chunk is processed and saved as a separate CSV file. This is performed using a loop that enumerates over `pd.read_csv`, which is configured to read the file in segments specified by `chunksize`.\n",
    "\n",
    "3. **Conditional Chunk Handling**:\n",
    "   - If the loop reaches the last specified chunk (as determined by `number_of_chunks - 1`), it will save all remaining rows in that chunk and then break the loop to stop further processing. This ensures that the process does not exceed the desired number of chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6e151-4442-4d55-a03d-5a78deeaa1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "input_file = 'note_Jun2023.csv'\n",
    "chunk_size = sum(1 for row in open(input_file, 'r')) \n",
    "number_of_chunks = 5\n",
    "\n",
    "# Process and save chunks\n",
    "for i, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size)):\n",
    "    if i == number_of_chunks - 1:  # If it's the last chunk, save all remaining rows\n",
    "        chunk.to_csv(f'chunk_{i}.csv', index=False)\n",
    "        break\n",
    "    chunk.to_csv(f'chunk_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c83bc7-ec12-4355-9d78-cceca1b3c16f",
   "metadata": {},
   "source": [
    "## Cleaning EHR chunk and group sentences (to meet BERT's rule of 512 tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a68da1-17cd-458c-8ae5-b5840d565272",
   "metadata": {},
   "source": [
    "### Random Sampling and Shuffling Data with Pandas\n",
    "\n",
    "This guide outlines the process of reading data from a CSV file into a DataFrame, shuffling the data, and obtaining a random sample. This approach is commonly used in data science to ensure model robustness by randomizing the order of data and reducing dataset size for manageable processing or experimental reproducibility.\n",
    "\n",
    "#### Libraries:\n",
    "- **pandas**: Used for data manipulation and analysis.\n",
    "\n",
    "#### Detailed Steps:\n",
    "\n",
    "1. **Reading Data**:\n",
    "   - The CSV file named `chunk_0.csv` is read into a pandas DataFrame named `df`. This initial step loads the data into memory, making it ready for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5897c8d7-9b3a-45e4-85e4-9b60c568d563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"chunk_0.csv\")\n",
    "# Shuffle the DataFrame\n",
    "df1_shuffled = df1.sample(frac=1, random_state=42)  # Using random_state for reproducibility\n",
    "# Get a random sample of 1,000,000 rows\n",
    "df = df1_shuffled.sample(n=1000000, random_state=42)  # Using random_state for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f1f84-c713-4f19-8b3b-bf4d196689ad",
   "metadata": {},
   "source": [
    "### Text Processing Functions for Natural Language Processing\n",
    "\n",
    "This guide provides an overview of three essential Python functions designed for preparing text data in natural language processing (NLP) applications. These functions are particularly useful when dealing with models like BERT that require specific text formatting.\n",
    "\n",
    "\n",
    "#### Functions Overview:\n",
    "\n",
    "1. **Preprocess and Clean Text**:\n",
    "   - This function is designed to standardize text data, making it more uniform and easier for models to process. It involves converting all text to lowercase, removing non-ASCII characters, filtering out unwanted punctuations, and eliminating extra white spaces.\n",
    "\n",
    "2. **Sentence Tokenization**:\n",
    "   - Sentence tokenization involves splitting a block of text into its constituent sentences. This step is essential for understanding the structure of the text and for subsequent processing steps that may treat each sentence as a separate unit.\n",
    "\n",
    "3. **Group Sentences**:\n",
    "   - This process involves clustering sentences together based on certain criteria. It can be used to maintain context or improve the efficiency of NLP models by processing chunks of related sentences together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1a276-7de0-4ce7-ab9b-e4a3e453cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_clean_text(text):\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # Replace 3 or more consecutive non-alphanumeric characters with 1 white space\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]{3,}', ' ', text)\n",
    "    \n",
    "    # Replace 2 or more consecutive white spaces with 1 white space\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    return text.strip()  # Trim whitespace from beginning and end\n",
    " \n",
    "# Tokenize sentences using nltk's sent_tokenize\n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# Function to group sentences with a maximum length of 500 tokens (to meet BERT's rule of 512 tokens)\n",
    "def group_sentences(sentences):\n",
    "    grouped_sentences = []\n",
    "    current_group = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())\n",
    "\n",
    "        if current_length + sentence_length <= 500:\n",
    "            current_group.append(sentence)\n",
    "            current_length += sentence_length\n",
    "        else:\n",
    "            if current_group:\n",
    "                grouped_sentences.append(current_group)\n",
    "                current_group = [sentence]\n",
    "                current_length = sentence_length\n",
    "\n",
    "    if current_group:\n",
    "        grouped_sentences.append(current_group)\n",
    "\n",
    "    return grouped_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553026dd-c7ca-439e-8631-9228f76940d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize to maintain continuity, though in a single-file scenario, it might be less relevant\n",
    "current_sent_id = 1\n",
    "# Apply preprocessing function directly while filtering and cleaning text data\n",
    "df['NOTE_TXT'] = df['NOTE_TXT'].apply(preprocess_and_clean_text)\n",
    "# Filter out rows with non-string values or missing values and short texts\n",
    "df = df[df['NOTE_TXT'].apply(lambda x: isinstance(x, str) and len(x.strip()) >= 10)]\n",
    "df.dropna(subset=['NOTE_TXT'], inplace=True)\n",
    "# Apply sentence tokenization and grouping to the 'NOTE_TXT' column\n",
    "df['Sentences'] = df['NOTE_TXT'].apply(tokenize_sentences)\n",
    "df['GroupedSentences'] = df['Sentences'].apply(group_sentences)\n",
    "\n",
    "# Use a list comprehension for generating new rows\n",
    "new_rows = [{'Note_ID': row['Note_ID'], 'SentID': current_sent_id + idx, 'SentText': ' '.join(group)}\n",
    "            for _, row in df3.iterrows()\n",
    "            for idx, group in enumerate(row['GroupedSentences'])]\n",
    "current_sent_id += len(new_rows)\n",
    "\n",
    "# Create a new DataFrame for the processed sentences\n",
    "df_sent = pd.DataFrame(new_rows, columns=['Note_ID', 'SentID', 'SentText'])\n",
    "\n",
    "# Save the processed DataFrame to a new CSV file\n",
    "df_sent.to_csv(f\"1_million_cleaned_data\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
