{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a0c116-8784-4dab-8595-76fea7ea1682",
   "metadata": {},
   "source": [
    "## Fine-tuning Multiclassification 13 Cancer Symtoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a69fff5d-b23d-480b-aeed-276f7978638c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os # Standard library for file and directory operations\n",
    "import numpy as np # Scientific computing library for array objects\n",
    "import pandas as pd # Data manipulation and analysis library\n",
    "import torch  # PyTorch is a machine learning library for tensor operations and automatic differentiation\n",
    "import torch.nn as nn  # Neural network module\n",
    "import matplotlib.pyplot as plt  # Plotting library for creating static, interactive, and animated visualizations\n",
    "from sklearn.model_selection import train_test_split # Scikit-learn model selection for splitting data\n",
    "from torch.utils.data import DataLoader, TensorDataset  # DataLoader and TensorDataset for handling datasets in PyTorch\n",
    "\n",
    "# Transformers library for NLP models and utilities\n",
    "from transformers import (\n",
    "    AutoTokenizer,                 # Tokenizer for converting text to tokens\n",
    "    AutoModelForSequenceClassification, # Model for sequence classification tasks\n",
    "    AdamW,                         # Adam optimizer with weight decay fix\n",
    "    get_linear_schedule_with_warmup # Scheduler for learning rate adjustment\n",
    ")\n",
    "\n",
    "# Scikit-learn metrics module for model evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report,  # Detailed classification metrics\n",
    "    confusion_matrix,       # Compute the confusion matrix to evaluate accuracy\n",
    "    precision_recall_curve, # Compute precision-recall pairs for different probability thresholds\n",
    "    roc_auc_score,          # Compute the Area Under the Receiver Operating Characteristic Curve\n",
    "    roc_curve,              # Compute ROC curve points\n",
    "    auc                     # Compute the area under a curve\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b5d82-88e8-4fb5-a6e5-d9af43c27c5f",
   "metadata": {},
   "source": [
    "## Initialization Setup\n",
    "\n",
    "This section set up for saving output files.\n",
    "\n",
    "### Setup for Output Files\n",
    "\n",
    "- **Folder Name Specification:**\n",
    "  - A folder named `\"results_BERT_models\"` is designated to store all CSV output files from model evaluations or other outputs.\n",
    "  \n",
    "- **Folder Creation:**\n",
    "  - The specified folder is created if it does not already exist. This ensures that all output files have a dedicated storage location, preventing any loss of data and maintaining organization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f74eddc-d8b7-48c9-9d65-d9ef2023b993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the folder name where you want to save the CSV files\n",
    "folder_name = \"results_BERT_models\"\n",
    "\n",
    "# Create the folder if it doesn't already exist\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10981b00-7a53-49cc-9167-33dcb3b37751",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "This section outlines the steps taken to load and preprocess the datasets used in the analysis.\n",
    "\n",
    "### Loading the Datasets\n",
    "\n",
    "1. **Gold Standard Corpus:**\n",
    "   - The dataset `df_Gold_Standard_Corpous.csv` contains the gold standard annotations and is loaded into a DataFrame named `df`.\n",
    "2.  **External Data:**\n",
    "  - The dataset `df_gpt_external.csv` contains external data for validation or testing, is loaded into the DataFrame `df_external`.\n",
    "  \n",
    "3. **Cohort Dataset:**\n",
    "   - The `df_clean_merged_table.csv` file, which contains cohort data for labeling and identifying labels for each text document, is loaded into a DataFrame named `df_cohort_usecase`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "932a390e-dc75-48ee-921f-2ae8617fd75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('df_Gold_Standard_Corpous.csv')\n",
    "df_external = pd.read_csv('gpt_external.csv')\n",
    "df_cohort_usecase = pd.read_csv(\"df_clean_merged_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f2ae8-9075-46a5-87d6-e7a07517561e",
   "metadata": {},
   "source": [
    "### Data Preparation for Cohort Use Case\n",
    "\n",
    "This section explains how the code processes a DataFrame specifically for a cohort use case, focusing on symptom analysis.\n",
    "\n",
    "#### Libraries:\n",
    "- **Pandas**: Used for handling and manipulating the data in DataFrame format.\n",
    "\n",
    "#### Steps Involved:\n",
    "\n",
    "1. **Extracting Cohort Symptoms**:\n",
    "   - `cohort_symptoms` is derived from the columns of `df_cohort_usecase` starting from the fourth column onward. This is done using `df_cohort_usecase.columns[3:].tolist()`, which captures all columns after the first three as a list of symptoms.\n",
    "\n",
    "2. **Renaming Columns**:\n",
    "   - The `SentText` column in `df_cohort_usecase` is renamed to `Note` to standardize the column names or to reflect the content more accurately.\n",
    "\n",
    "3. **Adding New Columns**:\n",
    "   - A list `new_columns` is defined, containing the names of new symptoms to be added to the DataFrame. These symptoms include 'Fatigue', 'Depressed Mood', and several others, representing different health conditions relevant to the cohort analysis.\n",
    "\n",
    "4. **Initializing New Columns**:\n",
    "   - Each symptom in `new_columns` is added to `df_cohort_usecase` with an initial value of 0. This could represent the absence of these symptoms or a baseline value before actual data is populated. You can replace `0` with `None` or another appropriate default value depending on the context of the data and the analysis requirements.\n",
    "\n",
    "#### Code Usage:\n",
    "This script is tailored for situations where new data dimensions (symptoms) need to be integrated into an existing dataset, often in preparation for more extensive data analysis or machine learning modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ec25a07-fcc4-4ede-92e7-d9c17dd57323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cohort_symptoms = df_cohort_usecase.columns[3:].tolist()\n",
    "df_cohort_usecase = df_cohort_usecase.rename(columns={'SentText': 'Note'})\n",
    "\n",
    "# List of new columns to add\n",
    "new_columns = ['Fatigue', 'Depressed_Mood', 'Constipation', 'Anxiety', 'Swelling', 'Nausea',\n",
    "               'Appetite_Loss', 'Pain', 'Numbness', 'Impaired_Memory', 'Pruritus',\n",
    "               'Shortness_of_Breath', 'Disturbed_Sleep']\n",
    "\n",
    "# Initialize each new column with a default value, e.g., None or 0\n",
    "for column in new_columns:\n",
    "    df_cohort_usecase[column] = 0  # Replace None with your desired default value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811fe8c7-dcc3-4fff-aa99-fe5e474faaed",
   "metadata": {},
   "source": [
    "## Model Configuration and Data Preparation\n",
    "\n",
    "This section provides an overview of the initial setup for machine learning models, including defining model abbreviations, setting a seed for reproducibility, and splitting the data for training and testing purposes.\n",
    "\n",
    "### Model Names and Abbreviations\n",
    "\n",
    "A dictionary named `model_names` maps various model identifiers to their abbreviations, enhancing the readability and ease of reference throughout the project:\n",
    "\n",
    "- **BERT**: `bert-base-uncased`\n",
    "- **SpanBERT**: `SpanBERT/spanbert-large-cased`\n",
    "- **BioBERT**: `dmis-lab/biobert-v1.1`\n",
    "- **ClinicalBERT**: `emilyalsentzer/Bio_ClinicalBERT`\n",
    "- **SciBERT**: `allenai/scibert_scivocab_uncased`\n",
    "- **PubMedBERT**: `microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract`\n",
    "- **DistilBERT**: `distilbert-base-uncased`\n",
    "- **Symptom BERT**: `New_Bio-Clinical_BERT_finetuned`\n",
    "\n",
    "\n",
    "- **Seed Setting**: The seed is set to `42` to guarantee reproducibility.\n",
    "\n",
    "- **Split Dataset**: The dataset is organized into distinct training and testing sets to facilitate effective model training and performance evaluation:\n",
    "\n",
    "- **Symptom Label Extraction**:Symptom labels are extracted from the datasets to aid in model training and evaluation, ensuring that labels are consistently handled across different datasets:\n",
    "\n",
    "- Main Dataset Symptoms: Extracted from columns starting after the third column in df.\n",
    "- External Dataset Symptoms: Starting from the second column in df_external.\n",
    "- Cohort Dataset Symptoms: Starting from the fourth column in df_cohort_usecase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69306243-a5a2-44be-89b0-89fa37282f25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model names with abbreviations\n",
    "model_names = {\n",
    "    #\"bert-base-uncased\": \"BERT\",\n",
    "    #\"SpanBERT/spanbert-large-cased\": \"SpanBERT\",\n",
    "    #\"dmis-lab/biobert-v1.1\": \"BioBERT\",\n",
    "    #\"emilyalsentzer/Bio_ClinicalBERT\": \"Bio-ClinicalBERT\",\n",
    "    #\"allenai/scibert_scivocab_uncased\": \"SciBERT\",\n",
    "   # \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\": \"PubMedBERT\",\n",
    "   # \"distilbert-base-uncased\": \"DistilBERT\",\n",
    "    \"New_Bio-Clinical_BERT_finetuned\": \"Symptom_BERT\"\n",
    "}\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Assuming all columns after 'Note' in the dataframe are symptom labels\n",
    "symptoms = df.columns[3:].tolist()\n",
    "ex_symptoms = df_external.columns[1:].tolist()\n",
    "cohort_symptoms = df_cohort_usecase.columns[3:].tolist()\n",
    "num_symptoms = len(symptoms)\n",
    "num_symptoms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f863a7-2f08-43ad-bc69-7ec4917ad7ae",
   "metadata": {},
   "source": [
    "## Data Processing Function\n",
    "\n",
    "The `process_data` function is designed to prepare textual data for machine learning models, specifically tailored for use with NLP models that require tokenized input. Below is a summary of its functionality:\n",
    "\n",
    "### Parameters\n",
    "- `df`: The DataFrame containing the text data.\n",
    "- `labels_list`: A list of column names in `df` that contain the labels for the text data.\n",
    "- `tokenizer`: A tokenizer object, typically from the `transformers` library, used to convert text into a format suitable for model input.\n",
    "- `max_length`: The maximum length of the tokenized sequences. If texts are longer than this, they will be truncated to this length.\n",
    "\n",
    "### Process\n",
    "1. **Extract Texts and Labels**:\n",
    "   - Texts are extracted from the column `'Note'` of the DataFrame.\n",
    "   - Labels are obtained based on the `labels_list`, converted to numeric format, handling errors by coercion and filling missing values with zero.\n",
    "\n",
    "2. **Tokenization**:\n",
    "   - The extracted texts are tokenized using the provided `tokenizer`. The tokenizer pads or truncates the texts to ensure uniform length, specified by `max_length`.\n",
    "   - This process generates `input_ids` and `attention_mask` which are essential for models to understand which parts of the sequence are meaningful and which are padding.\n",
    "\n",
    "3. **Tensor Conversion**:\n",
    "   - The labels are converted to a tensor of type `float32`.\n",
    "   - `input_ids` and `attention_mask` are bundled with labels into a `TensorDataset`, which is a convenient format for loading data during model training or evaluation.\n",
    "\n",
    "### Output\n",
    "- The function returns a `TensorDataset` containing `input_ids`, `attention_mask`, and `labels_tensor`, ready to be used as input for training or inference in a PyTorch model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74d2f73e-c8ae-4d69-a056-6128631750ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data processing function\n",
    "def process_data(df,labels_list, tokenizer, max_length=512):\n",
    "    texts = df['Note'].tolist()\n",
    "    labels = df[labels_list].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    input_ids = encodings['input_ids']\n",
    "    attention_mask = encodings['attention_mask']\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "    dataset = TensorDataset(input_ids, attention_mask, labels_tensor)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df8d10-b49f-4877-860e-341ce28c4c5c",
   "metadata": {},
   "source": [
    "## Evaluation Function for Loss\n",
    "\n",
    "The `evaluate` function calculates the average loss of a model on a given dataset, useful for assessing the model's performance during or after training.\n",
    "\n",
    "### Parameters\n",
    "- `model`: The machine learning model to be evaluated.\n",
    "- `data_loader`: A DataLoader object that provides batches of data in the form `(input_ids, attention_mask, labels)` format.\n",
    "- `device`: The computational device (CPU or GPU) where the model computations are performed.\n",
    "- `loss_fct`: The loss function used to compute the discrepancy between predicted outputs and actual labels.\n",
    "\n",
    "### Process\n",
    "1. **Model Preparation**:\n",
    "   - The model is set to evaluation mode (`model.eval()`) which turns off specific layers and behaviors suited to training, like dropout layers, to ensure consistent predictions.\n",
    "\n",
    "2. **Batch Processing**:\n",
    "   - The function iterates over each batch provided by the `data_loader`.\n",
    "   - Each batch's components (`input_ids`, `attention_mask`, and `labels`) are moved to the specified `device`, ensuring that both model and data are on the same device to avoid errors.\n",
    "\n",
    "3. **Loss Computation**:\n",
    "   - The model processes the `input_ids` and `attention_mask` to generate logits (model outputs before activation).\n",
    "   - The specified `loss_fct` computes the loss between these logits and the actual labels.\n",
    "   - The loss for each batch is accumulated to calculate the total loss over all batches.\n",
    "\n",
    "4. **Average Loss Calculation**:\n",
    "   - After processing all batches, the average loss is computed by dividing the total loss by the number of batches in the `data_loader`.\n",
    "\n",
    "### Output\n",
    "- Returns the average loss over all batches in the provided `data_loader`, giving a single scalar value that represents the model's performance in terms of the specified loss function on the given dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "55c027c7-806d-441b-a0a3-58ab257cb207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation function for loss\n",
    "\n",
    "def evaluate(model, data_loader, device, loss_fct):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fct(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb9fc42-1b13-4040-865c-c630dd0c5b47",
   "metadata": {},
   "source": [
    "### Function: `generate_predictions`\n",
    "\n",
    "This function is designed to generate predictions and their corresponding true labels from a provided data loader. It is intended for use with a pre-trained PyTorch model and assumes a multi-label classification context.\n",
    "\n",
    "#### Parameters:\n",
    "- **data_loader** (`DataLoader`): A PyTorch DataLoader that iterates over the dataset, providing batches of input data.\n",
    "- **symptoms** (`list`): A list of symptoms or conditions that are being predicted. These are used to key the output dictionaries.\n",
    "- **model** (`torch.nn.Module`): The trained model that will generate predictions.\n",
    "- **device** (`torch.device`): The device (CPU or GPU) where the model computations will be performed.\n",
    "\n",
    "#### Returns:\n",
    "- **tuple** (`dict`, `dict`): A tuple of two dictionaries. The first dictionary contains lists of predictions for each symptom, while the second contains the corresponding true labels.\n",
    "\n",
    "#### Code Explanation:\n",
    "\n",
    "- **Model Preparation**:\n",
    "  - Sets the model to evaluation mode using `model.eval()`, which disables dropout and batch normalization during inference, ensuring consistent predictions.\n",
    "\n",
    "- **Initialization**:\n",
    "  - Initializes two dictionaries, `predictions` and `true_labels`, which will store the predicted values and actual labels for each symptom respectively. These dictionaries use symptoms as keys.\n",
    "\n",
    "- **Prediction Generation**:\n",
    "  - Iterates over each batch from the `data_loader`:\n",
    "    1. **Data Transfer**: Transfers input IDs and attention masks to the specified device to ensure computations are performed in the correct hardware context.\n",
    "    2. **Inference**: Feeds the inputs through the model to obtain logits, the raw model outputs.\n",
    "    3. **Data Conversion**: Converts the logits and labels from PyTorch tensors to numpy arrays for easier manipulation.\n",
    "    4. **Data Aggregation**: Updates the `predictions` and `true_labels` dictionaries with the predicted and true values for each symptom, converting logits to lists of predicted probabilities.\n",
    "\n",
    "- **Output**:\n",
    "  - Once all batches are processed, the function returns the `predictions` and `true_labels` dictionaries, which now contain the complete set of predictions and actual labels for each symptom across all data.\n",
    "\n",
    "#### Example Usage:\n",
    "\n",
    "```python\n",
    "# Assuming model, data_loader, device, and symptoms list are predefined\n",
    "predictions, true_labels = generate_predictions(data_loader, symptoms, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1b024ac-c335-4ed2-9574-ac248d8818a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_predictions(data_loader, symptoms, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize dictionaries to store predictions and true labels for each symptom\n",
    "    predictions = {symptom: [] for symptom in symptoms}\n",
    "    true_labels = {symptom: [] for symptom in symptoms}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].cpu().numpy()  # Convert labels tensor to numpy array\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.cpu().numpy()  # Convert logits tensor to numpy array\n",
    "\n",
    "            # Update predictions and true labels for each symptom\n",
    "            for idx, symptom in enumerate(symptoms):\n",
    "                predictions[symptom].extend(logits[:, idx].tolist())\n",
    "                true_labels[symptom].extend(labels[:, idx].tolist())\n",
    "                \n",
    "    return predictions, true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c49ef-85e5-4bd2-81bf-decd6b892ccf",
   "metadata": {},
   "source": [
    "### Function: `calculate_metrics`\n",
    "\n",
    "This function computes various classification metrics for each symptom in a given dataset. It handles binary classification metrics, including precision, recall (sensitivity), F1-score, specificity, accuracy, AUPRC (Area Under the Precision-Recall Curve), and AUC (Area Under the Receiver Operating Characteristic Curve).\n",
    "\n",
    "#### Parameters:\n",
    "- **predictions** (`dict`): A dictionary where each key is a symptom and the associated value is a list of predicted probabilities for that symptom.\n",
    "- **true_labels** (`dict`): A dictionary matching the `predictions` structure but containing the actual binary labels for each symptom.\n",
    "- **symptoms** (`list`): A list of strings representing the symptoms for which metrics are calculated.\n",
    "\n",
    "#### Returns:\n",
    "- **dict**: A dictionary of dictionaries, where each top-level key is a metric name and each second-level key is a symptom, with the corresponding metric value.\n",
    "\n",
    "#### Code Explanation:\n",
    "\n",
    "- **Initialization**:\n",
    "  - A dictionary called `metrics` is initialized to store the calculated metrics. Each metric type has its own nested dictionary keyed by symptoms.\n",
    "\n",
    "- **Metric Calculation**:\n",
    "  - Iterates over each symptom provided in the `symptoms` list:\n",
    "    1. **Binarization of Predictions**: Converts predicted probabilities into binary outcomes based on a threshold of 0.5, where predictions equal to or above this threshold are treated as positive (1) and others as negative (0).\n",
    "    2. **Confusion Matrix**: Utilizes `confusion_matrix` to derive true negatives (tn), false positives (fp), false negatives (fn), and true positives (tp) from the binarized predictions and true labels.\n",
    "    3. **Calculation of Specific Metrics**:\n",
    "       - **Specificity**: Computed as tn / (tn + fp).\n",
    "       - **Accuracy**: Computed as (tp + tn) / (tp + tn + fp + fn).\n",
    "       - **Classification Report**: Generates a detailed report including precision, recall, and F1-score using `classification_report` with `output_dict=True` for easy extraction of values.\n",
    "    4. **AUPRC and AUC**:\n",
    "       - **AUPRC**: Calculated using `precision_recall_curve` and `auc` to determine the area under the curve based on the precision and recall values.\n",
    "       - **AUC**: Calculated using `roc_auc_score` which provides a statistical measure of how well predictions are capable of distinguishing between classes.\n",
    "\n",
    "- **Return**:\n",
    "  - The fully populated `metrics` dictionary is returned, containing all metrics for each symptom, allowing for a comprehensive assessment of model performance across multiple diagnostic categories.\n",
    "\n",
    "#### Example Usage:\n",
    "\n",
    "```python\n",
    "# Assuming predictions, true_labels, and symptoms are predefined\n",
    "metrics = calculate_metrics(predictions, true_labels, symptoms)\n",
    "for symptom, metric_values in metrics.items():\n",
    "    print(f\"Metrics for {symptom}:\")\n",
    "    for metric, value in metric_values.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "64f439f4-5dd7-4e9f-b0d4-89d68ad59ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, true_labels, symptoms):\n",
    "    # Initialize dictionaries to store metrics for each symptom\n",
    "    metrics = {\n",
    "        'precision': {},\n",
    "        'recall': {},\n",
    "        'f1_score': {},\n",
    "        'specificity': {},\n",
    "        'accuracy': {},\n",
    "        'auprc': {},\n",
    "        'auc': {}\n",
    "    }\n",
    "\n",
    "    for symptom in symptoms:\n",
    "        # Binarize predictions (considering 0.5 as threshold for demonstration)\n",
    "        binarized_predictions = [1 if pred >= 0.5 else 0 for pred in predictions[symptom]]\n",
    "        tn, fp, fn, tp = confusion_matrix(true_labels[symptom], binarized_predictions).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        report = classification_report(true_labels[symptom], binarized_predictions, output_dict=True, zero_division=0)\n",
    "        metrics['precision'][symptom] = report['weighted avg']['precision']\n",
    "        metrics['recall'][symptom] = report['weighted avg']['recall']  # Same as sensitivity\n",
    "        metrics['f1_score'][symptom] = report['weighted avg']['f1-score']\n",
    "        metrics['specificity'][symptom] = specificity\n",
    "        metrics['accuracy'][symptom] = accuracy\n",
    "        precision, recall, _ = precision_recall_curve(true_labels[symptom], predictions[symptom])\n",
    "        metrics['auprc'][symptom] = auc(recall, precision)\n",
    "        metrics['auc'][symptom] = roc_auc_score(true_labels[symptom], predictions[symptom])\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bae0a5-df8b-4e78-9d1b-efb79e16b083",
   "metadata": {},
   "source": [
    "### Function: `fine_tune_model`\n",
    "\n",
    "This function fine-tunes a pre-trained model for a specific task using training and validation data loaders. It is structured to work with PyTorch models and leverages optimizers and schedulers from the Hugging Face Transformers library.\n",
    "\n",
    "#### Parameters:\n",
    "- **model** (`torch.nn.Module`): The model to be fine-tuned.\n",
    "- **train_loader** (`DataLoader`): DataLoader for the training set, supplying batches of data.\n",
    "- **val_loader** (`DataLoader`): DataLoader for the validation set, used for evaluating model performance after each epoch.\n",
    "- **device** (`torch.device`): The device (CPU or GPU) on which the model computations are performed.\n",
    "- **epochs** (`int`): Number of total epochs to run the training.\n",
    "- **learning_rate** (`float`): Initial learning rate for the optimizer.\n",
    "- **weight_decay** (`float`): Weight decay coefficient for L2 penalty (regularization).\n",
    "\n",
    "#### Workflow:\n",
    "1. **Optimizer Initialization**:\n",
    "   - An `AdamW` optimizer is created with parameters from the model, along with specified learning rate and weight decay. This optimizer is known for combining the benefits of both Adam optimization and L2 regularization.\n",
    "\n",
    "2. **Scheduler Setup**:\n",
    "   - A linear schedule with warmup is configured using `get_linear_schedule_with_warmup`. It gradually increases the learning rate from 0 to the initial rate over a number of warmup steps (here set to 0), then linearly decreases it over the total training steps.\n",
    "\n",
    "3. **Training Loop**:\n",
    "   - For each epoch:\n",
    "     - Sets the model to training mode.\n",
    "     - Initializes `total_loss` to zero.\n",
    "     - Iterates over each batch in the `train_loader`:\n",
    "       - Transfers input IDs, attention masks, and labels to the specified device.\n",
    "       - Performs a forward pass to compute logits.\n",
    "       - Calculates loss using a predefined loss function.\n",
    "       - Performs backpropagation and updates the model parameters.\n",
    "       - Clips gradients to a maximum norm of 1.0 to prevent exploding gradients.\n",
    "       - Steps the optimizer and the scheduler to update learning rate.\n",
    "     - After processing all batches, computes the average loss for the epoch.\n",
    "\n",
    "4. **Validation**:\n",
    "   - At the end of each epoch, evaluates the model on the validation set using a separate function `evaluate`. This step helps monitor overfitting and adjust training dynamically.\n",
    "\n",
    "5. **Logging**:\n",
    "   - Prints the training and validation loss for each epoch to track progress and performance improvements.\n",
    "\n",
    "#### Example Usage:\n",
    "\n",
    "```python\n",
    "# Assuming model, train_loader, val_loader, and device are predefined\n",
    "epochs = 5\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 0.01\n",
    "fine_tune_model(model, train_loader, val_loader, device, epochs, learning_rate, weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "028d50c5-3ef5-4bd9-8a99-db22cd07911f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine-tuning function\n",
    "def fine_tune_model(model, train_loader, val_loader, device, epochs, learning_rate, weight_decay):\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = loss_fct(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # Evaluation on the validation set\n",
    "        val_loss = evaluate(model, val_loader, device, loss_fct)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {total_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990ea1e-6c66-4c90-8ebc-7aedde45613a",
   "metadata": {},
   "source": [
    "### Model Fine-Tuning and Evaluation Workflow\n",
    "\n",
    "The provided script outlines a comprehensive workflow for fine-tuning, evaluating, and generating predictions with machine learning models, particularly for sequence classification tasks using the Hugging Face Transformers library.\n",
    "\n",
    "#### Workflow Overview:\n",
    "\n",
    "1. **Model and Tokenizer Initialization**:\n",
    "   - For each model specified in the `model_names` dictionary:\n",
    "     - A tokenizer is initialized using `AutoTokenizer.from_pretrained`, specifying the model's name.\n",
    "     - A model is initialized with `AutoModelForSequenceClassification.from_pretrained`, configured to classify a number of labels equal to the length of `symptoms`.\n",
    "\n",
    "2. **Data Processing**:\n",
    "   - Data is prepared by processing datasets (`train_df`, `test_df`, `df_external`, and `df_cohort_usecase`) into formats suitable for training and evaluation, using a custom function `process_data`.\n",
    "\n",
    "3. **Data Loader Setup**:\n",
    "   - DataLoaders for training, testing, external evaluation, and cohort analysis are created with defined batch sizes and shuffle settings.\n",
    "\n",
    "4. **Device Configuration**:\n",
    "   - Determines if CUDA is available and sets the device accordingly, ensuring that models and computations are moved to GPU if available.\n",
    "\n",
    "5. **Optimization and Loss Configuration**:\n",
    "   - Configures the AdamW optimizer with specific learning rates and weight decay for regularization.\n",
    "   - Sets a binary cross-entropy loss function for the sequence classification tasks.\n",
    "\n",
    "6. **Model Fine-Tuning**:\n",
    "   - Calls `fine_tune_model` for actual training and validation using defined parameters, optimizers, and data loaders.\n",
    "\n",
    "7. **Model Evaluation**:\n",
    "   - Evaluates the fine-tuned model on test,and external to calculate losses.\n",
    "\n",
    "8. **Prediction and Metrics Calculation**:\n",
    "   - Generates predictions for each dataset using `generate_predictions`.\n",
    "   - Calculates classification metrics using `calculate_metrics`, assessing model performance across various metrics such as precision, recall, F1 score, and AUC.\n",
    "\n",
    "9. **Metrics Storage and Output**:\n",
    "   - Converts metrics into pandas DataFrames for easy viewing and analysis.\n",
    "   - Prints and saves the metrics to CSV files, facilitating further analysis and reporting.\n",
    "\n",
    "#### Example Console Outputs:\n",
    "- The script logs the progress of model fine-tuning and displays losses and metrics, ensuring that the user is informed of the model performance at every step.\n",
    "- After processing, it outputs DataFrames summarizing the precision, recall, F1 score, and AUC for each dataset, providing a clear and structured presentation of results.\n",
    "\n",
    "#### Usage of External Functions:\n",
    "- `fine_tune_model`, `evaluate`, `generate_predictions`, and `calculate_metrics` are assumed to be defined externally, performing specific tasks as described in the workflow.\n",
    "\n",
    "#### Example of Metrics Output:\n",
    "```python\n",
    "print(\"Test evaluation\")\n",
    "print(test_df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f43d5f36-b49e-4f3c-bcad-7ff2ad5ce4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine-tuning model: Symptom_BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at New_Bio-Clinical_BERT_finetuned and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/nzeinali/.local/lib/python3.8/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.3245, Val Loss: 0.5173\n",
      "Epoch 2 Loss: 0.2223, Val Loss: 0.4702\n",
      "Epoch 3 Loss: 0.1555, Val Loss: 0.5240\n",
      "Epoch 4 Loss: 0.1227, Val Loss: 0.5355\n",
      "Epoch 5 Loss: 0.1063, Val Loss: 0.5078\n",
      "Test Loss: 0.1358\n",
      "External Test Loss: 0.5078\n",
      "*********************************************************************************************\n",
      "Test evaluation\n",
      "                symptom  Test Precision  Test Recall  Test F1 Score  Test AUC\n",
      "0               Fatigue        0.956578     0.958140       0.956907  0.981894\n",
      "1        Depressed_Mood        0.911246     0.920930       0.913724  0.882180\n",
      "2          Constipation        0.986869     0.986047       0.986362  0.997073\n",
      "3               Anxiety        0.893738     0.893023       0.868514  0.898843\n",
      "4              Swelling        0.919302     0.925581       0.917233  0.900842\n",
      "5                Nausea        0.926233     0.930233       0.921375  0.833474\n",
      "6         Appetite_Loss        0.986251     0.986047       0.985188  0.969535\n",
      "7                  Pain        0.941993     0.944186       0.942416  0.971635\n",
      "8              Numbness        0.986247     0.986047       0.984702  0.926106\n",
      "9       Impaired_Memory        0.995372     0.995349       0.995262  0.948591\n",
      "10             Pruritus        0.986643     0.986047       0.986270  0.996802\n",
      "11  Shortness_of_Breath        0.959995     0.958140       0.952906  0.871379\n",
      "12      Disturbed_Sleep        0.981760     0.981395       0.980056  0.932000\n",
      "Saved test metrics to results_BERT_models/Symptom_BERT_test_metrics.csv\n",
      "*********************************************************************************************\n",
      "Extenal_Validation\n",
      "                symptom  Ext_Precision  Ex_Recall  Ex_F1 Score    Ex_AUC\n",
      "0               Fatigue       0.811830   0.747191     0.649634  0.715121\n",
      "1        Depressed_Mood       0.899552   0.910112     0.900654  0.926600\n",
      "2          Constipation       0.880205   0.859551     0.822304  0.886311\n",
      "3               Anxiety       0.662731   0.808989     0.728592  0.804807\n",
      "4              Swelling       0.784984   0.780899     0.707775  0.665616\n",
      "5                Nausea       0.601060   0.775281     0.677144  0.812862\n",
      "6         Appetite_Loss       0.768085   0.876404     0.818677  0.831585\n",
      "7                  Pain       0.741599   0.432584     0.377049  0.719848\n",
      "8              Numbness       0.748517   0.865169     0.802626  0.792478\n",
      "9       Impaired_Memory       0.904717   0.893258     0.847998  0.791772\n",
      "10             Pruritus       0.777376   0.876404     0.823925  0.910221\n",
      "11  Shortness_of_Breath       0.876142   0.853933     0.812470  0.841588\n",
      "12      Disturbed_Sleep       0.875511   0.853933     0.796461  0.669048\n",
      "Saved test metrics to results_BERT_models/Symptom_BERT_external_metrics.csv\n",
      "*********************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# For each model in the dictionary\n",
    "for model_name, abbreviation in model_names.items():\n",
    "    print(f\"fine-tuning model: {abbreviation}\")\n",
    "\n",
    "    # Initialize Model and Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_symptoms)\n",
    "    # Assuming you are using a model from the transformers library and need to set the number of output labels\n",
    "   \n",
    "    # Data Processing\n",
    "    train_dataset = process_data(train_df, symptoms, tokenizer)\n",
    "    test_dataset = process_data(test_df, symptoms, tokenizer)\n",
    "    external_dataset = process_data(df_external, ex_symptoms, tokenizer)\n",
    "    cohort_dataset = process_data(df_cohort_usecase, cohort_symptoms, tokenizer)\n",
    "    \n",
    "    # Define the data loaders\n",
    "    batch_size = 4\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    external_loader = DataLoader(external_dataset, batch_size=batch_size, shuffle=False)\n",
    "    cohort_loader=DataLoader(cohort_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the optimizer and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fct = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Fine-tuning and evaluation call\n",
    "    fine_tune_model(model, train_loader, external_loader, device, epochs=5, learning_rate=3e-5, weight_decay=0.01)\n",
    "\n",
    "    # After fine-tuning, evaluate on test, external datasets, and cohort dataset\n",
    "    test_loss = evaluate(model, test_loader, device, loss_fct)\n",
    "    external_loss = evaluate(model, external_loader, device, loss_fct)\n",
    "    cohort_loss = evaluate(model, cohort_loader, device, loss_fct)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"External Test Loss: {external_loss:.4f}\")\n",
    "    print(f\"*********************************************************************************************\")\n",
    "    \n",
    "    # Generate predictions and metrics for the test set\n",
    "    test_predictions, test_true_labels = generate_predictions(test_loader, symptoms, model, device)\n",
    "    test_metrics = calculate_metrics(test_predictions, test_true_labels, symptoms)\n",
    "    # Generate predictions and metrics for the external set\n",
    "    external_predictions, external_true_labels = generate_predictions(external_loader, ex_symptoms, model, device)\n",
    "    external_metrics = calculate_metrics(external_predictions, external_true_labels, ex_symptoms)\n",
    "    # Generate predictions and metrics for the cohort datset\n",
    "    cohort_predictions, cohort_true_labels = generate_predictions(cohort_loader, cohort_symptoms, model, device)\n",
    "    \n",
    "    \n",
    "    # Convert dictionaries to DataFrames for a tabular view\n",
    "    test_df_metrics = pd.DataFrame({\n",
    "    'symptom': symptoms,\n",
    "    'Test Precision': [test_metrics['precision'][s] for s in symptoms],\n",
    "    'Test Recall': [test_metrics['recall'][s] for s in symptoms],\n",
    "    'Test F1 Score': [test_metrics['f1_score'][s] for s in symptoms],\n",
    "    'Test AUC': [test_metrics['auc'][s] for s in symptoms]\n",
    "     })\n",
    "    print(\"Test evaluation\")\n",
    "    print(test_df_metrics)\n",
    "    metrics_filename = os.path.join(folder_name, f\"{abbreviation}_test_metrics.csv\")\n",
    "    test_df_metrics.to_csv(metrics_filename, index=False)\n",
    "    print(f\"Saved test metrics to {metrics_filename}\")\n",
    "    print(f\"*********************************************************************************************\")\n",
    "    \n",
    "    external_df_metrics = pd.DataFrame({\n",
    "    'symptom': symptoms,  # Assuming external_loader uses the same  structure\n",
    "    'Ext_Precision': [external_metrics['precision'][s] for s in symptoms],\n",
    "    'Ex_Recall': [external_metrics['recall'][s] for s in symptoms],\n",
    "    'Ex_F1 Score': [external_metrics['f1_score'][s] for s in symptoms],\n",
    "    'Ex_AUC': [external_metrics['auc'][s] for s in symptoms]\n",
    "    })\n",
    "    print(\"Extenal_Validation\")\n",
    "    print (external_df_metrics)\n",
    "    metrics_filename = os.path.join(folder_name, f\"{abbreviation}_external_metrics.csv\")\n",
    "    external_df_metrics.to_csv(metrics_filename, index=False)\n",
    "    print(f\"Saved test metrics to {metrics_filename}\")\n",
    "    print(f\"*********************************************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd2b06-dae3-45d1-b0a9-e258d7e9dbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865be2c7-b405-4958-8c26-7ff05a57ebd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef73801-dda0-409e-becf-e3eb43d79f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "# Initialize dictionaries to store predictions and true labels for each symptom\n",
    "predictions = {symptom: [] for symptom in symptoms}\n",
    "true_labels = {symptom: [] for symptom in symptoms}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].cpu().numpy()  # Convert labels tensor to numpy array\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.cpu().numpy()  # Convert logits tensor to numpy array\n",
    "\n",
    "        # Update predictions and true labels for each symptom\n",
    "        for idx, symptom in enumerate(symptoms):\n",
    "            predictions[symptom].extend(logits[:, idx].tolist())  # Get predicted probabilities for symptom\n",
    "            true_labels[symptom].extend(labels[:, idx].tolist())\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for symptom in symptoms:\n",
    "    # Calculate ROC curve and AUC for each symptom\n",
    "    fpr, tpr, _ = roc_curve(true_labels[symptom], predictions[symptom])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{symptom} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    # Binarize predictions for precision, recall, F1-score calculations\n",
    "    binarized_predictions = [1 if pred >= 0.5 else 0 for pred in predictions[symptom]]\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic for Each Symptom with RoBERTa')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa410a2-43a9-414f-960a-a0cdbf2baeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e845883-e8f3-4804-a17e-6b0faef80c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a11b2-8dd5-4176-9be5-2f176c5851b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309808b-441f-4b1d-8639-efc3b24f6f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0462a27-4a9e-481b-a576-7346eec3bbd8",
   "metadata": {},
   "source": [
    "## Labeling cohort study based on the Symtom-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351006f-666f-4750-a60e-b10cca9ec3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a new DataFrame by selecting relevant columns directly\n",
    "df_cohort_results = df_cohort[['Note_ID', 'SentID']].copy()\n",
    "df_cohort_results['NOTE_TXT'] = df_cohort['Note']\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Sigmoid function to convert logits to probabilities, assuming x is already a numpy array\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Assuming cohort_predictions are arrays, apply sigmoid function directly\n",
    "probabilities = {key: sigmoid(np.array(value)) for key, value in cohort_predictions.items()}\n",
    "\n",
    "def to_binary_labels(probabilities, threshold):\n",
    "    # Convert probabilities to binary labels using a threshold\n",
    "    return (probabilities >= threshold).astype(int)\n",
    "\n",
    "# Convert probabilities to binary labels with a threshold of 0.5\n",
    "binary_labels_50 = {key: to_binary_labels(value, 0.5) for key, value in probabilities.items()}\n",
    "\n",
    "# Append predictions directly to df_cohort_results without looping over each symptom\n",
    "for symptom in symptoms:\n",
    "    df_cohort_results[f'{symptom}_pred_50'] = binary_labels_50[symptom]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_cohort_results.to_csv('Case_Sentences_BERTLabels.csv', index=False)\n",
    "print(\"DataFrame saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3755ec-44b0-4b9f-ba4a-84553f7e7a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
